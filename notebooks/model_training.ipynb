{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates steps of creating a pipeline of model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- %pip install -q transformers datasets evaluate -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OopsWrongCode/nlp-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nlp-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from datasets import Dataset\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/working/nlp-project/data/train.csv')\n",
    "test = pd.read_csv('/kaggle/working/nlp-project/data/test.csv')\n",
    "validation = pd.read_csv('/kaggle/working/nlp-project/data/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token_count'] = [len(sentence.split()) for sentence in train['text']]\n",
    "train['text_length'] = [len(seq) for seq in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_types = train['label'].unique().tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(mbti_types)\n",
    "\n",
    "train['label'] = label_encoder.transform(train['label'])\n",
    "validation['label'] = label_encoder.transform(validation['label'])\n",
    "test['label'] = label_encoder.transform(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),  # (max_len)\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),  # (max_len)\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),  # (max_len)\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),  # (max_len)\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max len = {np.max(train['token_count'])}\\nMin len = {np.min(train['token_count'])}\\nAvg len = {np.round(np.mean(train['token_count']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = np.max(train['token_count'])\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = MBTIDataset(texts=train['text'].tolist(), labels=train['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "test_dataset = MBTIDataset(texts=test['text'].tolist(),labels=test['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "validation_dataset = MBTIDataset(texts=validation['text'].tolist(), labels=validation['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularization, we employ two commonly used\n",
    "techniques: dropout (Hinton et al., 2012) and L2\n",
    "weight regularization. We apply dropout to prevent co-adaptation. In our model, we either apply\n",
    "dropout to word vectors before feeding the sequence\n",
    "of words into the convolutional layer or to the output\n",
    "of LSTM before the softmax layer. The L2 regularization is applied to the weight of the softmax layer. (https://arxiv.org/pdf/1511.08630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim=6, num_layers=1, bidirectional=False, dropout=0.3, fc_dropout=0.3, input_dropout=0.2):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(input_dropout) # \n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, # 768\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # LAYER 2: Fully-connected\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim * (2 if bidirectional else 1),\n",
    "            output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, bert_embeddings):  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # Dropout on BERT-embeddings\n",
    "        x = self.input_dropout(bert_embeddings)\n",
    "\n",
    "        lstm_output, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        h_final = self.fc_dropout(h_final)\n",
    "        out = self.fc(h_final)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim=6, num_layers=1, bidirectional=False, dropout=0.3, fc_dropout=0.1):\n",
    "        super(MyGRU, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        gru_output, h_n = self.gru(x)\n",
    "\n",
    "        if self.gru.bidirectional:\n",
    "            h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        h_final = self.fc_dropout(h_final)\n",
    "        out = self.fc(h_final)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-LSTM: https://arxiv.org/pdf/1511.08630\n",
    "\n",
    "class HybridNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, conv_out_channels=256, kernel_size=3, \n",
    "                 hidden_dim=256, output_dim=6, lstm_layers=2, bidirectional=True, \n",
    "                 dropout=0.4, fc_dropout=0.2, input_dropout=0.1):\n",
    "        super(HybridNN, self).__init__()\n",
    "\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "\n",
    "        # CNN block\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim,\n",
    "                               out_channels=conv_out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv_out_channels,\n",
    "                               out_channels=conv_out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=1)\n",
    "\n",
    "        self.spatial_dropout = nn.Dropout2d(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=conv_out_channels,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout if lstm_layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.attn_linear = nn.Linear(hidden_dim * (2 if bidirectional else 1), 1)\n",
    "\n",
    "        # Output\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Dropout input\n",
    "        x = self.input_dropout(embeddings)  # [batch_size, seq_len, embedding_dim]\n",
    "        # Conv1D\n",
    "        x = x.permute(0, 2, 1)                 # [B, seq, emb]\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)                 # LSTM\n",
    "        x = self.spatial_dropout(x) \n",
    "\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_dim*2]\n",
    "\n",
    "        #attention mechanism\n",
    "        attn_weights = torch.softmax(self.attn_linear(lstm_out), dim=1)  # [batch_size, seq_len, 1]\n",
    "        attn_output = torch.sum(lstm_out * attn_weights, dim=1)  # [batch_size, hidden_dim*2]\n",
    "\n",
    "        # FC\n",
    "        out = self.fc_dropout(attn_output)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train.label), y=train.label)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights[5] = class_weights[5] * 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "gru_model = MyGRU(embedding_dim=768, hidden_dim=128, output_dim=6, num_layers=5, dropout=0.3,fc_dropout=0.2, bidirectional=True).to(DEVICE)\n",
    "\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "gru_scheduler = ReduceLROnPlateau(gru_optimizer, patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights[5] = class_weights[5] * 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = MyLSTM(embedding_dim=768, hidden_dim=128, output_dim=6, num_layers=2, dropout=0.5, bidirectional=True).to(DEVICE)\n",
    "\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# lstm_scheduler = ReduceLROnPlateau(lstm_optimizer, patience=3, factor=1e-2)\n",
    "lstm_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(lstm_optimizer, T_0=5, T_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = HybridNN(embedding_dim=768, conv_out_channels=128, kernel_size=4, \n",
    "                 hidden_dim=128, output_dim=6, lstm_layers=2, bidirectional=True, \n",
    "                 dropout=0.4, fc_dropout=0.2, input_dropout=0.1).to(DEVICE)\n",
    "\n",
    "hybrid_optimizer = optim.Adam(hybrid_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "hybrid_scheduler = ReduceLROnPlateau(hybrid_optimizer, patience=3, factor=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, model, patience=3, min_delta=0.01, restore_best_weights=True, save_weights=False):\n",
    "        self.model = model\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.best_weights = None\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.save_weights = save_weights\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss - self.min_delta:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.restore_best_weights and self.best_weights is not None:\n",
    "                    self.model.load_state_dict(self.best_weights)\n",
    "                    if self.save_weights:\n",
    "                        torch.save(self.best_weights, f'{self.model.__class__.__name__}_best_weights.pt')\n",
    "                        print(f\"Weights has been saved\")\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "# source: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertLSTMConvHybrid(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, lstm_hidden_dim=128, conv_out_dim=128,\n",
    "                 num_classes=6, lstm_layers=1, dropout=0.5):\n",
    "        super(BertLSTMConvHybrid, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=lstm_layers,\n",
    "                             dropout=dropout if lstm_layers > 1 else 0,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=2 * lstm_hidden_dim, out_channels=conv_out_dim,\n",
    "                               kernel_size=10, stride=3)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(conv_out_dim, lstm_hidden_dim, num_layers=lstm_layers,\n",
    "                             dropout=dropout if lstm_layers > 1 else 0,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=2 * lstm_hidden_dim, out_channels=conv_out_dim,\n",
    "                               kernel_size=10, stride=3)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.fc = nn.Linear(conv_out_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x = BERT embeddings [batch, seq_len, 768]\n",
    "        x, _ = self.lstm1(x)  # → [batch, seq_len, 2*hidden]\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # → [batch, channels, seq_len]\n",
    "        x = F.relu(self.conv1(x))  # → conv1\n",
    "        x = x.permute(0, 2, 1)  # → back to [batch, seq_len, channels]\n",
    "\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv2(x))  # → conv2\n",
    "\n",
    "        x = self.global_max_pool(x)  # → [batch, conv_out_dim, 1]\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Epoch 1/25 | Train Loss: 0.7673 | Val Loss: 0.3724 | Val Acc: 0.7462\n",
      "🌀 Epoch 2/25 | Train Loss: 0.3147 | Val Loss: 0.3488 | Val Acc: 0.7738\n",
      "🌀 Epoch 3/25 | Train Loss: 0.2432 | Val Loss: 0.3327 | Val Acc: 0.7903\n",
      "🌀 Epoch 4/25 | Train Loss: 0.1745 | Val Loss: 0.3679 | Val Acc: 0.8023\n",
      "🌀 Epoch 5/25 | Train Loss: 0.1239 | Val Loss: 0.3915 | Val Acc: 0.8108\n",
      "🌀 Epoch 6/25 | Train Loss: 0.0955 | Val Loss: 0.4035 | Val Acc: 0.8103\n",
      "🚨 Early stopping at epoch 7\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       274\n",
      "           1       0.68      0.76      0.72       212\n",
      "           2       0.91      0.80      0.85       703\n",
      "           3       0.53      0.80      0.64       178\n",
      "           4       0.94      0.80      0.86       550\n",
      "           5       0.58      0.88      0.70        81\n",
      "\n",
      "    accuracy                           0.80      1998\n",
      "   macro avg       0.73      0.81      0.76      1998\n",
      "weighted avg       0.83      0.80      0.81      1998\n",
      "\n",
      "✅ Train finished\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# === UNFREEZE ===\n",
    "for name, param in bert.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name or \"pooler\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# === FOCAL LOSS ===\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "\n",
    "# === SETUP ===\n",
    "EPOCHS = 25\n",
    "lstm_model = MyLSTM(embedding_dim=768, hidden_dim=128, output_dim=6, num_layers=2, dropout=0.5, bidirectional=True, fc_dropout=0.3, input_dropout=0.2).to(DEVICE)\n",
    "MODEL = BertLSTMConvHybrid(\n",
    "    embedding_dim=768,\n",
    "    lstm_hidden_dim=128,\n",
    "    conv_out_dim=128,\n",
    "    num_classes=6,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.5\n",
    ").to(DEVICE)\n",
    "\n",
    "OPTIMIZER = AdamW(list(MODEL.parameters()) + list(bert.parameters()), lr=2e-5, weight_decay=1e-4)\n",
    "SCHEDULER = CosineAnnealingWarmRestarts(OPTIMIZER, T_0=5, T_mult=2)\n",
    "early_stopper = EarlyStopper(patience=4, model=MODEL, min_delta=0.01, save_weights=False)\n",
    "\n",
    "lr_history = []\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "for epoch in range(EPOCHS):\n",
    "    MODEL.train()\n",
    "    bert.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # BERT embedding\n",
    "        bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = bert_output.last_hidden_state\n",
    "\n",
    "        outputs = MODEL(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    MODEL.eval()\n",
    "    bert.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state\n",
    "\n",
    "            outputs = MODEL(embeddings)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(validation_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    SCHEDULER.step(epoch)  # cosine uses epoch, not val_loss\n",
    "    current_lr = OPTIMIZER.param_groups[0]['lr']\n",
    "    lr_history.append(current_lr)\n",
    "\n",
    "    if early_stopper.early_stop(avg_val_loss):\n",
    "        print(f\"🚨 Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    print(f\"🌀 Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# === FINAL REPORT ===\n",
    "class_report = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(6)])\n",
    "print(f\"\\n📊 Classification Report:\\n{class_report}\")\n",
    "print(\"✅ Train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 25\n",
    "MODEL = lstm_model\n",
    "OPTIMIZER = lstm_optimizer\n",
    "SCHEDULER = lstm_scheduler\n",
    "lr_history = []\n",
    "early_stopper = EarlyStopper(patience=4, model=MODEL, min_delta=0.01, save_weights=False)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    MODEL.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        OPTIMIZER.zero_grad() # replace\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():  # BERT embed\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        outputs = MODEL(embeddings) #replace\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step() #replace\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    MODEL.eval() #replace\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state\n",
    "\n",
    "            outputs = MODEL(embeddings) #replace\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(validation_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    SCHEDULER.step(avg_val_loss) # replace \n",
    "    current_lr = OPTIMIZER.param_groups[0]['lr'] #replace\n",
    "    lr_history.append(current_lr)\n",
    "\n",
    "    if early_stopper.early_stop(avg_val_loss):             \n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} — Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "class_report = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(6)])\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "print(\"Train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# EPOCHS = 25\n",
    "# MODEL = hybrid_model\n",
    "# OPTIMIZER = hybrid_optimizer\n",
    "# SCHEDULER = hybrid_scheduler\n",
    "# lr_history = []\n",
    "# early_stopper = EarlyStopper(patience=4, model=MODEL, min_delta=0.01, save_weights=False)\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     MODEL.train()\n",
    "#     bert.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         OPTIMIZER.zero_grad() # replace\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(DEVICE)\n",
    "#         attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "#         labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "#         bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         embeddings = bert_output.last_hidden_state\n",
    "\n",
    "#         outputs = MODEL(embeddings) #replace\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         OPTIMIZER.step() #replace\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     # === VALIDATION ===\n",
    "#     MODEL.eval() #replace\n",
    "#     val_preds = []\n",
    "#     val_labels = []\n",
    "#     val_loss_total = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in validation_loader:\n",
    "#             input_ids = batch['input_ids'].to(DEVICE)\n",
    "#             attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "#             labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "#             bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             embeddings = bert_output.last_hidden_state\n",
    "\n",
    "#             outputs = MODEL(embeddings) #replace\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             val_loss_total += loss.item()\n",
    "\n",
    "#             preds = torch.argmax(outputs, dim=1)\n",
    "#             val_preds.extend(preds.cpu().numpy())\n",
    "#             val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     avg_val_loss = val_loss_total / len(validation_loader)\n",
    "#     val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "#     SCHEDULER.step(avg_val_loss) # replace \n",
    "#     current_lr = OPTIMIZER.param_groups[0]['lr'] #replace\n",
    "#     lr_history.append(current_lr)\n",
    "\n",
    "#     if early_stopper.early_stop(avg_val_loss):             \n",
    "#         print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#         break\n",
    "\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS} — Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# class_report = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(6)])\n",
    "# print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# print(\"Train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights loaded after training\n"
     ]
    }
   ],
   "source": [
    "if early_stopper.best_weights is not None:\n",
    "    MODEL.load_state_dict(early_stopper.best_weights)\n",
    "    print(\"Best weights loaded after training\")\n",
    "else:\n",
    "    print(\"No best weights were saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4016 | Validation Accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "MODEL.load_state_dict(early_stopper.best_weights)\n",
    "MODEL.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = bert_output.last_hidden_state\n",
    "\n",
    "        outputs = MODEL(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss_total += loss.item()\n",
    "\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "avg_test_loss = test_loss_total / len(test_loader)\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Validation Loss: {avg_test_loss:.4f} | Validation Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(lr_history) + 1), lr_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
