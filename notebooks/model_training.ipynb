{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates steps of creating a pipeline of model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- %pip install -q transformers datasets evaluate -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OopsWrongCode/nlp-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nlp-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from datasets import Dataset\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/working/nlp-project/data/train.csv')\n",
    "test = pd.read_csv('/kaggle/working/nlp-project/data/test.csv')\n",
    "validation = pd.read_csv('/kaggle/working/nlp-project/data/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token_count'] = [len(sentence.split()) for sentence in train['text']]\n",
    "train['text_length'] = [len(seq) for seq in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_types = train['label'].unique().tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(mbti_types)\n",
    "\n",
    "train['label'] = label_encoder.transform(train['label'])\n",
    "validation['label'] = label_encoder.transform(validation['label'])\n",
    "test['label'] = label_encoder.transform(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),  # (max_len)\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),  # (max_len)\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),  # (max_len)\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),  # (max_len)\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max len = {np.max(train['token_count'])}\\nMin len = {np.min(train['token_count'])}\\nAvg len = {np.round(np.mean(train['token_count']), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = np.max(train['token_count'])\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = MBTIDataset(texts=train['text'].tolist(), labels=train['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "test_dataset = MBTIDataset(texts=test['text'].tolist(),labels=test['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "validation_dataset = MBTIDataset(texts=validation['text'].tolist(), labels=validation['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularization, we employ two commonly used\n",
    "techniques: dropout (Hinton et al., 2012) and L2\n",
    "weight regularization. We apply dropout to prevent co-adaptation. In our model, we either apply\n",
    "dropout to word vectors before feeding the sequence\n",
    "of words into the convolutional layer or to the output\n",
    "of LSTM before the softmax layer. The L2 regularization is applied to the weight of the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim=6, num_layers=1, bidirectional=False, dropout=0.3, fc_dropout=0.3, input_dropout=0.1):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(input_dropout) # \n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, # 768\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # LAYER 2: Fully-connected\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim * (2 if bidirectional else 1),\n",
    "            output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, bert_embeddings):  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # Dropout on  BERT-embeddings\n",
    "        x = self.input_dropout(bert_embeddings)\n",
    "\n",
    "        lstm_output, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        h_final = self.fc_dropout(h_final)\n",
    "        out = self.fc(h_final)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim=6, num_layers=1, bidirectional=False, dropout=0.3, fc_dropout=0.2):\n",
    "        super(MyGRU, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        gru_output, h_n = self.gru(x)\n",
    "\n",
    "        if self.gru.bidirectional:\n",
    "            h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "\n",
    "        h_final = self.fc_dropout(h_final)\n",
    "        out = self.fc(h_final)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "gru_model = MyGRU(embedding_dim=768, hidden_dim=128, output_dim=6, num_layers=5, dropout=0.3, bidirectional=True).to(DEVICE)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train.label), y=train.label)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Обновляем loss function с учетом весов\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "gru_scheduler = ReduceLROnPlateau(gru_optimizer, patience=3, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights[5] = class_weights[5] * 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = MyLSTM(embedding_dim=768, hidden_dim=128, output_dim=6, num_layers=3, dropout=0.5, bidirectional=True).to(DEVICE)\n",
    "\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "lstm_scheduler = ReduceLROnPlateau(lstm_optimizer, patience=3, factor=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, model, patience=3, min_delta=0, restore_best_weights=True, save_weights=False):\n",
    "        self.model = model\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.best_weights = None\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.save_weights = save_weights\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss - self.min_delta:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.restore_best_weights and self.best_weights is not None:\n",
    "                    self.model.load_state_dict(self.best_weights)\n",
    "                    if self.save_weights:\n",
    "                        torch.save(self.best_weights, f'{self.model.__class__.__name__}_best_weights.pt')\n",
    "                        print(f\"Weights has been saved\")\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "# source: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 16\n",
    "MODEL = lstm_model\n",
    "OPTIMIZER = lstm_optimizer\n",
    "lr_history = []\n",
    "early_stopper = EarlyStopper(patience=3, model=MODEL, min_delta=0.01, save_weights=False)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    MODEL.train() #replace\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        OPTIMIZER.zero_grad() # replace\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():  # BERT embed\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        outputs = MODEL(embeddings) #replace\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step() #replace\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    MODEL.eval() #replace\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state\n",
    "\n",
    "            outputs = MODEL(embeddings) #replace\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(validation_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    lstm_scheduler.step(avg_val_loss) # replace \n",
    "    current_lr = OPTIMIZER.param_groups[0]['lr'] #replace\n",
    "    lr_history.append(current_lr)\n",
    "\n",
    "    if early_stopper.early_stop(avg_val_loss):             \n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} — Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "class_report = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(6)])\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "print(\"Train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if early_stopper.best_weights is not None:\n",
    "    MODEL.load_state_dict(early_stopper.best_weights)\n",
    "    print(\"Best weights loaded after training\")\n",
    "else:\n",
    "    print(\"No best weights were saved (maybe early stop didn’t trigger?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.load_state_dict(early_stopper.best_weights)\n",
    "MODEL.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = bert_output.last_hidden_state\n",
    "\n",
    "        outputs = MODEL(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss_total += loss.item()\n",
    "\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "avg_test_loss = test_loss_total / len(test_loader)\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Validation Loss: {avg_test_loss:.4f} | Validation Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(lr_history) + 1), lr_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-LSTM: https://arxiv.org/pdf/1511.08630\n",
    "\n",
    "\n",
    "class HybridNN:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
